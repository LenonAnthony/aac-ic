{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets peft transformers trl pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3384738752.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip3 freeze > requirements2.txt\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "from transformers import GenerationConfig\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "output_model = \"tinyllama-papagaio-v10\"\n",
    "\n",
    "def formatted_train(input, response):\n",
    "    return f\"<|user|>\\n{input}</s>\\n<|assistant|>\\n{response}</s>\"\n",
    "\n",
    "df = pd.read_csv('livoxdataset.csv')\n",
    "\n",
    "def prepare_train_data(df):\n",
    "    df = df.drop(columns=['instruction'])\n",
    "    df['output'] = df['output'].apply(lambda x: x.replace('\\n', ' '))\n",
    "    df[\"text\"] = df.apply(lambda row: f\"<|user|>\\n{row['input']}</s>\\n<|assistant|>\\n{row['output']}\", axis=1)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset\n",
    "\n",
    "data = prepare_train_data(df)\n",
    "\n",
    "def get_model_and_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(model_id)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.07, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_model,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=16,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=1e-3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=1200,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    max_seq_length=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtenção de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(user_input):\n",
    "    prompt = formatted_train(user_input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    generation_config = GenerationConfig(penalty_alpha=0.9, do_sample=True,\n",
    "                                      top_k=10, temperature=0.45, repetition_penalty=1.2,\n",
    "                                      max_new_tokens=100, pad_token_id=tokenizer.eos_token_id\n",
    "                                      )\n",
    "    start_time = perf_counter()\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    output_time = perf_counter() - start_time\n",
    "    print(f\"Time taken for inference: {round(output_time, 2)} seconds\")\n",
    "\n",
    "def generate_responses(df):\n",
    "    outputs = []\n",
    "    for row in df.iterrows():\n",
    "        prompt = formatted_train(row['input'])\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        generation_config = GenerationConfig(penalty_alpha=0.9, do_sample=True,\n",
    "                                          top_k=10, temperature=0.45, repetition_penalty=1.2,\n",
    "                                          max_new_tokens=100, pad_token_id=tokenizer.eos_token_id\n",
    "                                          )\n",
    "        outputs.append(tokenizer.decode(model.generate(**inputs, generation_config=generation_config)[0], skip_special_tokens=True))\n",
    "    df['output_model_without_lora'] = outputs\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('livoxdataset.csv')\n",
    "\n",
    "df = generate_responses(df)\n",
    "\n",
    "df.to_csv('livoxdataset_without_lora_outputs.csv', index=False)\n",
    "\n",
    "model_path = \"tinyllama-papagaio-v10/checkpoint-1200\"\n",
    "peft_model = PeftModel.from_pretrained(model, model_path, from_transformers=True, device_map=\"auto\")\n",
    "model = peft_model.merge_and_unload()\n",
    "\n",
    "df = generate_responses(df)\n",
    "\n",
    "df.to_csv('livoxdataset_with_model_lora_outputs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtenção de Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "outputs_without_lora = pd.read_csv('livoxdataset_without_lora_outputs.csv')\n",
    "outputs_with_lora = pd.read_csv('livoxdataset_with_model_lora_outputs.csv')\n",
    "livoxdataset = pd.read_csv('livoxdataset.csv')\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    return sentence_bleu([reference], hypothesis)\n",
    "\n",
    "outputs_without_lora['BLEU_without_LORA'] = outputs_without_lora.apply(lambda row: calculate_bleu(livoxdataset.loc[row.name, 'output'], row['output_model_without_lora']), axis=1)\n",
    "\n",
    "outputs_with_lora['BLEU_with_LORA'] = outputs_with_lora.apply(lambda row: calculate_bleu(livoxdataset.loc[row.name, 'output'], row['output_model_with_lora']), axis=1)\n",
    "\n",
    "outputs_without_lora.to_csv('outputs_without_lora.csv', index=False)\n",
    "outputs_with_lora.to_csv('outputs_with_lora.csv', index=False)\n",
    "\n",
    "media_com_lora = outputs_with_lora['BLEU_with_LORA'].mean()\n",
    "media_sem_lora = outputs_without_lora['BLEU_without_LORA'].mean()\n",
    "\n",
    "print(f\"Média com LORA: {media_com_lora:.4f}\")\n",
    "print(f\"Média sem LORA: {media_sem_lora:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "import pandas as pd \n",
    "from sacrebleu.metrics import TER\n",
    "\n",
    "outputs_without_lora = pd.read_csv('livoxdataset_without_lora_outputs.csv')\n",
    "outputs_with_lora = pd.read_csv('livoxdataset_with_lora_outputs.csv')\n",
    "livoxdataset = pd.read_csv('livoxdataset.csv')\n",
    "\n",
    "ter_metric = TER()\n",
    "\n",
    "def calculate_ter(reference, hypothesis):\n",
    "    return ter_metric.sentence_score(hypothesis, [reference]).score\n",
    "\n",
    "outputs_without_lora['TER_without_LORA'] = outputs_without_lora.apply(\n",
    "    lambda row: calculate_ter(livoxdataset.loc[row.name, 'output'], row['output_model_without_lora']), axis=1)\n",
    "\n",
    "outputs_with_lora['TER_with_LORA'] = outputs_with_lora.apply(\n",
    "    lambda row: calculate_ter(livoxdataset.loc[row.name, 'output'], row['output_model_with_lora']), axis=1)\n",
    "\n",
    "media_ter_com_lora = outputs_with_lora['TER_with_LORA'].mean()\n",
    "media_ter_sem_lora = outputs_without_lora['TER_without_LORA'].mean()\n",
    "\n",
    "print(f\"Média TER com LORA: {media_ter_com_lora:.4f}\")\n",
    "print(f\"Média TER sem LORA: {media_ter_sem_lora:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
